Selection Sort: (In-place algorithm)
	Time Complexity		= O(n^2)
	Space Complexity  = O(1)

Insertion Sort: (In-place algorithm)
	Time Complexity		= O(n^2)
	Space Complexity	= O(1)

Bubble Sort: (In-place algorithm)
	Time Complexity		= O(n^2)
	Space Complexity 	= O(1)

Merge Sort: (Not in-place algorithm)
	Time Complexity		= O(n*logn)
	Space Complexity 	= O(n*logn)
	
	Time Complexity:
		- Length of array A is n.
		- Assignment of left and right subarray takes n units of time.
		- Calling mergeSort recursion function takes T(n/2) units of time.
		- Calling merge function takes n units of time.
		- When n = 1, the base condition is true for which we return the function, and it takes T(1) (Constant) units of time.
		- T(n) = T(1) (Constant), for n < 2
		- T(n) = 2*T(n/2) + n, otherwise
		- T(n/2) = 2*T(n/4) + n/2 (We can write this for any n)
		- This will make T(n) (in general terms for some k):
					T(n) = (2^k)*T(n/(2^k)) + k*n
		- For our base condition, T(n) = constant. This implies that:
				=>	n/(2^k) = 1
				=>	n = 2^k
				=>	k = logn (with base 2)
		- Substituting k in T(n) to get T(n) in terms of constant and n:
				T(n) = n*logn + n
				which is equivalent to O(n*logn).
	Space Complexity:
		- We are dividing any array into left and right subarray and at each recursive call, we are alloting extra memory to 
			these left and right subarrays.
		- For any level of recursive call, the alloted memory to left and right subarray will be n/(2^k) units of memory.
		- At the last level of recursive call, the alloted memory will be 1 unit of memory to both.
				=> n/(2^k) = 1
				=> k = logn (with base 2) (This much memory is allotted to the subarrays created)
		- This means that we will be having logn levels for any n.
		- And for every integer stored in these subarrays, we will require n units of memory.
		- Thus, we will ne needing n*logn memory units if we donot clear the allotted memory to these subarrays.
		- If we clear the memory alloted to the left and right sub array, then sum of all the allotted memory to these left 
			and right subarray will be:
				=> n + n/2 + n/3 + ......... upto infinity
				=> n(1 + 1/2 + 1/4 + 1/8 + ....... upto infinity)
				=> 2*n
		- Thus the space complexity will be O(n).
		
Quick Sort:
	
	Time Complexity (Average Case and Best Case)	= O(n*logn) (with base 2)
	TIme Complexity (Worst Case) = O(n^2)
	Space Complexity (Average case)	= O(logn) (With Base 2)
	Space Complexity (Worst Case) = O(n)
	
	Time Complexity:
		- First, lets calculate the time complexity of the partition function.
		- In partition function, we have a for loop from startInd till (ndInd - 1), which is the length of the array A passed.
		- Lets say length of A is n. Thus, the time complexity of the partition function is (a*n + b), where a and b are constants.
			For Best Case:
				- Now, in quickSort function, every recursive call is going to have half the index values for startInd and endInd.
				- Every recursive call is going to call the partitioning function for which we have calculated the time complexity at the beginning.
				- So, lets say T(n) is the total time complexity of the the recursive calls made. Then T(n) can be written as:
									T(n) = T(n/2) + T(n/2) + c*n, where  T(n/2) represents the time complexity for both left and right partitions and c*n 
																								comes from the partition function a*n + b collective constants.
						and,	T(1) = m(constant)
				- We know we can write T(n/2) in terms of T(n/4), thus writing T(n) in terms of T(n/(2^k)):
									T(n) = (2^k)T(n/(2^k)) + k*c*n
				- Now, writing T(n) in terms of T(1), for which we have to equate n/(2^k) = 1, we get k = logn, with base 2.
							=> T(n) = O(n*logn)
			For Worst Case:
				- There will be unequal partitioning, and even in that case, only one partition will exist, i.e., either the left partition or
				the right partition. And even in this case, the array will be already sorted in order for unequal partitioning to occur for
				for every partition function call.
				- So, only one recursive call will be made and the index size passed on every recursive call will be (n-1).
				- So, T(n) can be written as:
							T(n) = T(n-1) + c*n
							T(n) = T(n - k) + k*c*n (To write T(n) in terms of T(1))
							For n - k = 1, we will have k = n - 1
					=>	T(n) = T(1) + c*(n - 1)*n
					=> 	T(n) = O(n^2)
				- This worst case can be solved with a slight change of pivotInd selection. By changing the pivotInd in the partition function
				from endInd to some random index by calling a Random() function in the partition fucntion can resolve this issue, and improve
				this worst case.
			For Average Case:
				- As discussed above, Random() function will reduce this time complexity as:
							T(n) = (1/n)*(Summation(T(i - 1) + T(n - i)) + c*n
					=>  T(n) = O(n*logn)
		
